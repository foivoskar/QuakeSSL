{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Classify events with SSL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the waveform data are used for training an algorithm to identify the type of earthquake (strikeslip, normal, reverse) with a Self-Supervised Learning methodology, where we train it with a small sample of each size and then we ask to classify the event dataset based on this training.\n",
    "\n",
    "There are two options for the user:\n",
    "\n",
    "1) Using the same threshold for the identification of the events by their moment tensor (see Step 2) and feed the algorithm with about the 30% of the available data of each type, while asking to classify the rest 70% and the unclassified ones.\n",
    "2) Using different thresholds and add the lower threshold obtained data as feed (higher accuracy) and then ask to identify the data of the waveforms classified by their moment tensor with a higher threshold and also the unclassified ones for that threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© Foivos Karakostas - UCL, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable feedperc is the percentage of seismograms that are going to be used for each type of earthquake for training the algorithm against the total number of events existing in the given feed dataset. Use 0.3 or similar when the feed and classify thresholds are the same, use values up to 1 when they are different (the feed sample should be smaller than the classifiable, because otherwise the overlap is in all data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = 'Z'\n",
    "distance = 15\n",
    "backazimuth = 0\n",
    "feed_threshold = 20\n",
    "classify_threshold = 45\n",
    "feedperc = 0.6\n",
    "\n",
    "lf = 0.1\n",
    "hf = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "from obspy import read\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these functions for the pre-processing of the data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_waveform(dir, eventname, distance, backazimuth, component, lf, hf):\n",
    "    \n",
    "    directory = dir + '/' + eventname\n",
    "    filename = 'SY.SD' + str(distance).zfill(3) + 'B' + str(backazimuth).zfill(3) + '.mseed'\n",
    "    filepath = directory + '/' + filename\n",
    "    input = read(filepath)\n",
    "    if component == 'Z':\n",
    "        st = input[0]\n",
    "    elif component == 'R':\n",
    "        st = input[1]\n",
    "    elif component == 'T':\n",
    "        st = input[2]\n",
    "    st.filter('bandpass', freqmin = lf, freqmax = hf)\n",
    "    t = st.times();\n",
    "    d = st.data;    \n",
    "    \n",
    "    return t, d\n",
    "\n",
    "def remove_overlaps(feed_events, classify_events_init):\n",
    "\n",
    "    classify_events = []\n",
    "    for e in range(0, len(classify_events_init)):\n",
    "        event = classify_events_init[e]\n",
    "        check = 0\n",
    "        for j in range(0, len(feed_events)):\n",
    "            eventf = feed_events[j]\n",
    "            if event == eventf:\n",
    "                check = check + 1\n",
    "        if check == 0:\n",
    "            classify_events.append(event)\n",
    "   \n",
    "    return classify_events\n",
    "\n",
    "def take_feed_events(feed_events_init, feedperc):\n",
    "    feed_events = []\n",
    "    feedlength = math.ceil(len(feed_events_init)*feedperc)\n",
    "    for e in range(0, feedlength):\n",
    "        event = feed_events_init[e]\n",
    "        feed_events.append(event)\n",
    "\n",
    "    return feed_events\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Events 1 = 11 | Classify Events 1 = 15 | Total Events 1  = 26\n",
      "Feed Events 2 = 12 | Classify Events 2 = 36 | Total Events 2  = 48\n",
      "Feed Events 3 = 4 | Classify Events 3 = 22 | Total Events 3  = 26\n"
     ]
    }
   ],
   "source": [
    "syntdirectory_feed = 'Synthetics' + str(feed_threshold)\n",
    "syntdirectory1_feed = syntdirectory_feed + '/Strikeslip'\n",
    "syntdirectory2_feed = syntdirectory_feed + '/Normal'\n",
    "syntdirectory3_feed = syntdirectory_feed + '/Reverse'\n",
    "\n",
    "syntdirectory_classify = 'Synthetics' + str(classify_threshold)\n",
    "syntdirectory1_classify = syntdirectory_classify + '/Strikeslip'\n",
    "syntdirectory2_classify = syntdirectory_classify + '/Normal'\n",
    "syntdirectory3_classify = syntdirectory_classify + '/Reverse'\n",
    "syntdirectory4_classify = syntdirectory_classify + '/Unclassified'\n",
    "\n",
    "feed_events1_init = os.listdir(syntdirectory1_feed)\n",
    "feed_events2_init = os.listdir(syntdirectory2_feed)\n",
    "feed_events3_init = os.listdir(syntdirectory3_feed)\n",
    "\n",
    "classify_events1_init = os.listdir(syntdirectory1_classify)\n",
    "classify_events2_init = os.listdir(syntdirectory2_classify)\n",
    "classify_events3_init = os.listdir(syntdirectory3_classify)\n",
    "classify_events4_init = os.listdir(syntdirectory4_classify)\n",
    "\n",
    "feed_events1 = take_feed_events(feed_events1_init, feedperc)   \n",
    "feed_events2 = take_feed_events(feed_events2_init, feedperc)\n",
    "feed_events3 = take_feed_events(feed_events3_init, feedperc) \n",
    "    \n",
    "classify_events1 = remove_overlaps(feed_events1, classify_events1_init)\n",
    "classify_events2 = remove_overlaps(feed_events2, classify_events2_init)\n",
    "classify_events3 = remove_overlaps(feed_events3, classify_events3_init)\n",
    "classify_events4 = classify_events4_init\n",
    "\n",
    "message_1 = 'Feed Events 1 = ' + str(len(feed_events1)) + ' | Classify Events 1 = ' + str(len(classify_events1)) + ' | Total Events 1  = ' + str(len(classify_events1_init))\n",
    "print(message_1)\n",
    "message_2 = 'Feed Events 2 = ' + str(len(feed_events2)) + ' | Classify Events 2 = ' + str(len(classify_events2)) + ' | Total Events 2  = ' + str(len(classify_events2_init))\n",
    "print(message_2)\n",
    "message_3 = 'Feed Events 3 = ' + str(len(feed_events3)) + ' | Classify Events 3 = ' + str(len(classify_events3)) + ' | Total Events 3  = ' + str(len(classify_events3_init))\n",
    "print(message_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = []\n",
    "d_data = []\n",
    "labels = []\n",
    "\n",
    "for e in range(0, len(feed_events1)):\n",
    "    label = 1\n",
    "    event = feed_events1[e]\n",
    "    t,d = take_waveform(syntdirectory1_feed, event, distance, backazimuth, component, lf, hf)\n",
    "    t_data.append(t)\n",
    "    d_data.append(d)\n",
    "    labels.append(1)\n",
    "    \n",
    "for e in range(0, len(feed_events2)):\n",
    "    label = 2\n",
    "    event = feed_events2[e]\n",
    "    t,d = take_waveform(syntdirectory2_feed, event, distance, backazimuth, component, lf, hf)\n",
    "    t_data.append(t)\n",
    "    d_data.append(d)\n",
    "    labels.append(2)\n",
    "\n",
    "for e in range(0, len(feed_events3)):\n",
    "    label = 3\n",
    "    event = feed_events3[e]\n",
    "    t,d = take_waveform(syntdirectory3_feed, event, distance, backazimuth, component, lf, hf)\n",
    "    t_data.append(t)\n",
    "    d_data.append(d)\n",
    "    labels.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 77.78%\n",
      "F1 Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((t_data, d_data))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_waveform(t, d):\n",
    "    input_data = np.hstack((t, d)).reshape(1, -1)\n",
    "    input_data = scaler.transform(input_data)\n",
    "    \n",
    "    predicted_label = model.predict(input_data)[0]\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1: 7 occurrences\n",
      "Type 2: 2 occurrences\n",
      "Type 3: 6 occurrences\n"
     ]
    }
   ],
   "source": [
    "labels_for_1 = []\n",
    "\n",
    "for e in range(0, len(classify_events1)):\n",
    "    event = classify_events1[e]\n",
    "    new_t_data, new_d_data = take_waveform(syntdirectory1_classify,event,distance,backazimuth,component, lf, hf)\n",
    "    predicted_label = classify_waveform(new_t_data, new_d_data)\n",
    "    labels_for_1.append(predicted_label)\n",
    "    \n",
    "class_counts = Counter(labels_for_1)\n",
    "for event_type in range(1, 4):\n",
    "    count = class_counts.get(event_type, 0)\n",
    "    print(f\"Type {event_type}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1: 11 occurrences\n",
      "Type 2: 12 occurrences\n",
      "Type 3: 13 occurrences\n"
     ]
    }
   ],
   "source": [
    "labels_for_2 = []\n",
    "\n",
    "for e in range(0, len(classify_events2)):\n",
    "    event = classify_events2[e]\n",
    "    new_t_data, new_d_data = take_waveform(syntdirectory2_classify,event,distance,backazimuth,component, lf, hf)\n",
    "    predicted_label = classify_waveform(new_t_data, new_d_data)\n",
    "    labels_for_2.append(predicted_label)\n",
    "    \n",
    "class_counts = Counter(labels_for_2)\n",
    "for event_type in range(1, 4):\n",
    "    count = class_counts.get(event_type, 0)\n",
    "    print(f\"Type {event_type}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1: 4 occurrences\n",
      "Type 2: 1 occurrences\n",
      "Type 3: 17 occurrences\n"
     ]
    }
   ],
   "source": [
    "labels_for_3 = []\n",
    "\n",
    "for e in range(0, len(classify_events3)):\n",
    "    event = classify_events3[e]\n",
    "    new_t_data, new_d_data = take_waveform(syntdirectory3_classify,event,distance,backazimuth,component, lf, hf)\n",
    "    predicted_label = classify_waveform(new_t_data, new_d_data)\n",
    "    labels_for_3.append(predicted_label)\n",
    "    \n",
    "class_counts = Counter(labels_for_3)\n",
    "for event_type in range(1, 4):\n",
    "    count = class_counts.get(event_type, 0)\n",
    "    print(f\"Type {event_type}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1: 1 occurrences\n",
      "Type 2: 4 occurrences\n",
      "Type 3: 4 occurrences\n"
     ]
    }
   ],
   "source": [
    "labels_for_4 = []\n",
    "\n",
    "for e in range(0, len(classify_events4)):\n",
    "    event = classify_events4[e]\n",
    "    new_t_data, new_d_data = take_waveform(syntdirectory4_classify,event,distance,backazimuth,component, lf, hf)\n",
    "    predicted_label = classify_waveform(new_t_data, new_d_data)\n",
    "    labels_for_4.append(predicted_label)\n",
    "    \n",
    "class_counts = Counter(labels_for_4)\n",
    "for event_type in range(1, 4):\n",
    "    count = class_counts.get(event_type, 0)\n",
    "    print(f\"Type {event_type}: {count} occurrences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sismo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
